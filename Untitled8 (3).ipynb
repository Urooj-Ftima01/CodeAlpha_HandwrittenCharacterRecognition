{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Handwritten Character & Digit Recognition\n",
        "# Using CNN with TensorFlow/Keras\n",
        "# Supports MNIST (digits) & EMNIST (characters)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "\n",
        "# -------------------- STEP 1: Select Dataset --------------------\n",
        "dataset_choice = \"emnist\"  # Change to \"mnist\" or \"emnist\"\n",
        "\n",
        "if dataset_choice == \"mnist\":\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    num_classes = 10\n",
        "    input_shape = (28, 28, 1)\n",
        "\n",
        "elif dataset_choice == \"emnist\":\n",
        "    # EMNIST Letters via TensorFlow Datasets\n",
        "    import tensorflow_datasets as tfds\n",
        "    emnist_data, info = tfds.load('emnist/letters', with_info=True, as_supervised=True)\n",
        "    train_data, test_data = emnist_data['train'], emnist_data['test']\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_train, y_train = [], []\n",
        "    for img, label in tfds.as_numpy(train_data):\n",
        "        X_train.append(img)\n",
        "        y_train.append(label)\n",
        "    X_test, y_test = [], []\n",
        "    for img, label in tfds.as_numpy(test_data):\n",
        "        X_test.append(img)\n",
        "        y_test.append(label)\n",
        "\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "\n",
        "    num_classes = 27  # EMNIST letters dataset has 26 letters + 1 for indexing\n",
        "    input_shape = (28, 28, 1)\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Testing data shape:\", X_test.shape)\n",
        "\n",
        "# -------------------- STEP 2: Preprocess --------------------\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "# -------------------- STEP 3: Build CNN Model --------------------\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# -------------------- STEP 4: Train Model --------------------\n",
        "history = model.fit(X_train, y_train_cat, epochs=5, validation_split=0.1, batch_size=128)\n",
        "\n",
        "# -------------------- STEP 5: Evaluate Model --------------------\n",
        "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -------------------- STEP 6: Predictions & Confusion Matrix --------------------\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm, annot=False, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# -------------------- STEP 7: Show Sample Predictions --------------------\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in range(10):\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.imshow(X_test[i].reshape(28,28), cmap='gray')\n",
        "    plt.title(f\"Pred: {y_pred_classes[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T7rqyN7Faxqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}